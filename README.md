ðŸ“„ RAG Document Chatbot â€“ Assignment Submission

This project implements a Retrieval-Augmented Generation (RAG) Document Question-Answering Chatbot using:

FAISS vectorstore

Sentence-Transformer embeddings

Gemini 2.5 Flash (latest Google model)

LangChain pipeline

Custom LLM wrapper

PDF ingestion + chunking + semantic search

ðŸš€ Features
âœ” Ingest PDFs

Reads all PDFs in pdfs/

Extracts text

Splits into chunks

Generates embeddings

Saves FAISS index (index.faiss, index.pkl)

âœ” Ask questions about the documents

The chatbot retrieves relevant chunks from FAISS and generates an answer using Gemini.

âœ” Uses Gemini 2.5 Flash LLM

Free tier

Fast

High-performance

Does not break with model changes like Groq

ðŸ“Œ Installation
Step 1 â€” Create virtual environment
python -m venv .venv
.\.venv\Scripts\activate

Step 2 â€” Install dependencies
pip install -r requirements.txt

Step 3 â€” Set API Key

Edit config.py:

GEMINI_API_KEY = "your_key_here"
LLM_MODEL = "gemini-2.5-flash"


**Folder Structure**

RAG_PROJECT_2/
â”‚
â”œâ”€â”€ Q1/
â”‚   â”œâ”€â”€ Parse.py
â”‚   â”œâ”€â”€ sample_input.txt
â”‚
â”œâ”€â”€ Q2/
â”‚   â”œâ”€â”€ top_words.py
â”‚   â”œâ”€â”€ sample_input.txt
â”‚
â”œâ”€â”€ Q3_RAG_Document_Chatbot/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”œâ”€â”€ rag_chain.py
â”‚   â”œâ”€â”€ llm_provider.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ pdfs/
â”‚   â”‚   â””â”€â”€ your_documents.pdf
â”‚   â”‚
â”‚   â””â”€â”€ vectorstore/
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ index.pkl
â”‚
â””â”€â”€ screenshots.pdf
